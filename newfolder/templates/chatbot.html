<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chatbot Interview</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style_chatbot.css') }}">
</head>
<body>
    <div id="chatbot">
        <h1>Webcam Emotion Detection</h1>
        <video id="webcam" autoplay></video>
        <div class="controls">
            <button id="start">Start Detection</button>
            <button id="stop">Stop Detection</button>
        </div>
        <div id="emotion">Detected Emotion: None</div>

        <h2 id="question">{{ questions[0] }}</h2>
        <button id="start-btn" onclick="startRecording()" disabled>Start Recording</button>
        <button id="stop-btn" onclick="stopRecording()" disabled>Stop Recording</button>
        <div id="status"></div>
    </div>

    <script>
        let currentQuestion = 0;
        let recorder;
        let audioChunks = [];
        let isEmotionStopped = false; // Track if emotion detection has stopped

        const questions = {{ questions|tojson }}; // Questions passed from Flask
        const video = document.getElementById('webcam');
        const startButton = document.getElementById('start');
        const stopButton = document.getElementById('stop');
        const emotionDiv = document.getElementById('emotion');
        let stream;
        let interval;

        // Enable recording buttons after loading the page
        window.onload = () => {
            document.getElementById('start-btn').disabled = false;
        };

        async function startWebcam() {
            try {
                stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                video.style.width = "calc(100% - 10px)";  // Reduce width by 10px
                video.style.height = "calc(100% - 10px)"; // Reduce height by 10px
            } catch (err) {
                console.error("Error accessing webcam:", err);
            }
        }

        function stopWebcam() {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
                video.srcObject = null;
            }
        }

        function startDetection() {
            isEmotionStopped = false; // Reset when starting detection
            interval = setInterval(() => {
                const canvas = document.createElement('canvas');
                canvas.width = video.videoWidth - 10;
                canvas.height = video.videoHeight - 10;
                const context = canvas.getContext('2d');
                context.drawImage(video, 0, 0, canvas.width, canvas.height);
                const frame = canvas.toDataURL('image/jpeg');

                fetch('/analyze', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ frame })
                })
                .then(response => response.json())
                .then(data => {
                    emotionDiv.textContent = `Detected Emotion: ${data.emotion || 'None'}`;
                })
                .catch(err => console.error("Error analyzing frame:", err));
            }, 1000); // Send a frame every second
        }

        startButton.addEventListener('click', startDetection);
        stopButton.addEventListener('click', stopDetection);
        startWebcam();

        async function startRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                recorder = new MediaRecorder(stream);
                audioChunks = []; // Reset audio chunks for a new recording

                document.getElementById('start-btn').disabled = true;
                document.getElementById('stop-btn').disabled = false;
                document.getElementById('status').innerText = 'Recording...';

                recorder.ondataavailable = event => {
                    audioChunks.push(event.data);
                };

                recorder.onstop = async () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });

                    let userFilename = prompt("Enter a name for this recording:", `response_${currentQuestion + 1}`);
                    if (!userFilename || userFilename.trim() === '') {
                        userFilename = `response_${currentQuestion + 1}`;
                    }

                    if (!userFilename.endsWith('.wav')) {
                        userFilename += '.wav';
                    }

                    const formData = new FormData();
                    formData.append('audio', audioBlob, `response_${currentQuestion + 1}.wav`);
                    formData.append('question', questions[currentQuestion]);

                    document.getElementById('status').innerText = 'Uploading...';
                    document.getElementById('start-btn').disabled = true;
                    document.getElementById('stop-btn').disabled = true;
                    formData.append('filename', userFilename);

                    try {
                        const response = await fetch('/save-response', {
                            method: 'POST',
                            body: formData
                        });

                        const result = await response.text();
                        console.log(result);

                        document.getElementById('status').innerText = 'Audio saved successfully!';
                        nextQuestion();
                    } catch (err) {
                        console.error('Error uploading audio:', err);
                        document.getElementById('status').innerText = 'Failed to save audio. Please try again.';
                    }
                };

                recorder.start();
            } catch (err) {
                console.error('Error accessing microphone:', err);
                alert('Failed to access microphone. Please check your permissions.');
            }
        }

        function stopRecording() {
            if (recorder && recorder.state === 'recording') {
                recorder.stop();
                document.getElementById('status').innerText = 'Recording stopped. Saving audio...';
            } else {
                alert('No recording in progress.');
            }
        }

        function nextQuestion() {
    currentQuestion++;
    if (currentQuestion < questions.length) {
        // Update the question text
        document.getElementById('question').innerText = questions[currentQuestion];
        document.getElementById('start-btn').disabled = false; // Enable start button
        document.getElementById('status').innerText = '';
    } else {
        // End of questions, replace the question and buttons with analysis instruction
        document.getElementById('question').innerText = 'Click "Stop Detection" for analysis.';
        document.getElementById('start-btn').remove(); // Remove Start Recording button
        document.getElementById('stop-btn').remove(); // Remove Stop Recording button
        document.getElementById('status').innerText = ''; // Clear status text
    }
}

function stopDetection() {
    clearInterval(interval);
    emotionDiv.textContent = 'Detection stopped.';
    fetch('/save', { method: 'POST' }) // Save the last emotion to emo.txt
        .then(response => response.json())
        .then(data => {
            console.log(data.message);
            // After detection is stopped, move to transcription and thank-you message
            document.getElementById('chatbot').innerHTML = `
                <h2>Your Interview is completed! Thank you.</h2>
                <button id="transcribe-btn" onclick="transcribeAudio()">Transcribe</button>
                <div id="transcribe-status"></div>
                <button id="result-btn" onclick="viewResult()" disabled>View Results</button>
            `;
        })
        .catch(err => console.error("Error saving output:", err));
}

// Function to transcribe audio
async function transcribeAudio() {
    document.getElementById('transcribe-status').innerText = 'Transcribing...';

    try {
        const response = await fetch('/transcribe', {
            method: 'POST'
        });
        const result = await response.json();

        if (result.status === 'success') {
            document.getElementById('transcribe-status').innerText = 'All audios have been transcribed!';
            document.getElementById('result-btn').disabled = false;
        } else {
            document.getElementById('transcribe-status').innerText = 'Transcription failed. Try again.';
            document.getElementById('result-btn').disabled = true; // Disable button if transcription fails
        }
    } catch (err) {
        console.error('Error transcribing audio:', err);
        document.getElementById('transcribe-status').innerText = 'Transcription failed. Try again.';
    }
}

// Function to view results
async function viewResult() {
    try {
        const response = await fetch('/view_result', { method: 'POST' });
        const result = await response.json();

        if (result.status === 'success') {
            alert('Results have been generated!');
            sessionStorage.setItem('vocab_result', result.vocab_result);
            sessionStorage.setItem('confidence_result', result.confidence_result);
            window.location.href = '/result'; // Redirect to results page
        } else {
            console.error('Error details:', result.error);
            alert(`Failed to generate results: ${result.error}`);
        }
    } catch (err) {
        console.error('Error generating results:', err);
        alert('Failed to generate results. Check the backend for more details.');
    }
}
    </script>
</body>
</html>
 