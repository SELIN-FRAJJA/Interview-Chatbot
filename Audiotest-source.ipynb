{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Spectrogram\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths to your dataset\n",
    "CONFIDENT_FILE = os.path.join('confident_wav', 'Confident (1).wav')\n",
    "UNCONFIDENT_FILE = os.path.join('unconfident_wav', 'Unconfident (1).wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "def load_wav_16k_mono(filename):\n",
    "    # Load encoded wav file\n",
    "    waveform, sample_rate = torchaudio.load(filename)\n",
    "    # Resample to 16 kHz and convert to mono\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "    return waveform.mean(dim=0).unsqueeze(0)  # convert to mono and add batch dimension\n",
    "\n",
    "def preprocess(filename):\n",
    "    waveform = load_wav_16k_mono(filename)\n",
    "    spectrogram = Spectrogram(n_fft=320, hop_length=32)(waveform)\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, max_length=1500):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_paths: List of file paths for the audio files.\n",
    "            labels: List of corresponding labels (1 or 0).\n",
    "            max_length: Fixed length for spectrograms. Spectrograms longer than this will be truncated,\n",
    "                        and shorter ones will be padded.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spectrogram = self.load_spectrogram(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Truncate or pad spectrogram to max_length\n",
    "        if spectrogram.shape[2] > self.max_length:\n",
    "            spectrogram = spectrogram[:, :, :self.max_length]  # Truncate\n",
    "        elif spectrogram.shape[2] < self.max_length:\n",
    "            padding = self.max_length - spectrogram.shape[2]\n",
    "            spectrogram = F.pad(spectrogram, (0, padding), mode='constant', value=0)  # Pad\n",
    "\n",
    "        return spectrogram, label\n",
    "\n",
    "    def load_spectrogram(self, path):\n",
    "        \"\"\"\n",
    "        Loads the spectrogram from a file.\n",
    "        \"\"\"\n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "        spectrogram = Spectrogram(n_fft=320, hop_length=32)(waveform.mean(dim=0).unsqueeze(0))\n",
    "        return spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "confident_dir = os.path.join('confident_wav')\n",
    "unconfident_dir = os.path.join('unconfident_wav')\n",
    "\n",
    "confident_files = [os.path.join(confident_dir, file) for file in os.listdir(confident_dir) if file.endswith('.wav')]\n",
    "unconfident_files = [os.path.join(unconfident_dir, file) for file in os.listdir(unconfident_dir) if file.endswith('.wav')]\n",
    "\n",
    "file_paths = confident_files + unconfident_files\n",
    "labels = [1] * len(confident_files) + [0] * len(unconfident_files)\n",
    "\n",
    "dataset = AudioDataset(file_paths, labels, max_length=1500)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = None  # Placeholder for dynamic initialization\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(x.shape[1], 128).to(x.device)  # Dynamically initialize fc1\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio_path):\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Convert stereo to mono by averaging channels if there are 2 channels\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    # Create a Mel spectrogram\n",
    "    spectrogram = torchaudio.transforms.MelSpectrogram()(waveform)\n",
    "    spectrogram = spectrogram.unsqueeze(0)  # Add batch dimension\n",
    "    return spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.8765, Accuracy: 44.19%, Val Loss: 0.9110, Val Accuracy: 36.36%\n",
      "Epoch [2/30], Loss: 0.6701, Accuracy: 58.91%, Val Loss: 0.8522, Val Accuracy: 24.24%\n",
      "Epoch [3/30], Loss: 0.6287, Accuracy: 62.79%, Val Loss: 0.8261, Val Accuracy: 39.39%\n",
      "Epoch [4/30], Loss: 0.5823, Accuracy: 61.24%, Val Loss: 0.8430, Val Accuracy: 42.42%\n",
      "Epoch [5/30], Loss: 0.5696, Accuracy: 63.57%, Val Loss: 0.8776, Val Accuracy: 48.48%\n",
      "Epoch [6/30], Loss: 0.5600, Accuracy: 70.54%, Val Loss: 0.9236, Val Accuracy: 48.48%\n",
      "Epoch [7/30], Loss: 0.5119, Accuracy: 75.19%, Val Loss: 0.9581, Val Accuracy: 48.48%\n",
      "Epoch [8/30], Loss: 0.4993, Accuracy: 72.09%, Val Loss: 1.0526, Val Accuracy: 48.48%\n",
      "Epoch [9/30], Loss: 0.4525, Accuracy: 72.87%, Val Loss: 1.0565, Val Accuracy: 45.45%\n",
      "Epoch [10/30], Loss: 0.4284, Accuracy: 75.97%, Val Loss: 1.0681, Val Accuracy: 45.45%\n",
      "Epoch [11/30], Loss: 0.4061, Accuracy: 76.74%, Val Loss: 1.0751, Val Accuracy: 45.45%\n",
      "Epoch [12/30], Loss: 0.3911, Accuracy: 77.52%, Val Loss: 1.0924, Val Accuracy: 48.48%\n",
      "Epoch [13/30], Loss: 0.4240, Accuracy: 80.62%, Val Loss: 1.1479, Val Accuracy: 45.45%\n",
      "Epoch [14/30], Loss: 0.3510, Accuracy: 80.62%, Val Loss: 1.1721, Val Accuracy: 48.48%\n",
      "Epoch [15/30], Loss: 0.3493, Accuracy: 86.05%, Val Loss: 1.1971, Val Accuracy: 45.45%\n",
      "Epoch [16/30], Loss: 0.3153, Accuracy: 88.37%, Val Loss: 1.2551, Val Accuracy: 48.48%\n",
      "Epoch [17/30], Loss: 0.3042, Accuracy: 86.05%, Val Loss: 1.3101, Val Accuracy: 48.48%\n",
      "Epoch [18/30], Loss: 0.3073, Accuracy: 89.92%, Val Loss: 1.3665, Val Accuracy: 45.45%\n",
      "Epoch [19/30], Loss: 0.2808, Accuracy: 89.15%, Val Loss: 1.4157, Val Accuracy: 51.52%\n",
      "Epoch [20/30], Loss: 0.2616, Accuracy: 90.70%, Val Loss: 1.4416, Val Accuracy: 42.42%\n",
      "Epoch [21/30], Loss: 0.2647, Accuracy: 91.47%, Val Loss: 1.5240, Val Accuracy: 42.42%\n",
      "Epoch [22/30], Loss: 0.2467, Accuracy: 93.02%, Val Loss: 1.5711, Val Accuracy: 42.42%\n",
      "Epoch [23/30], Loss: 0.2186, Accuracy: 92.25%, Val Loss: 1.6433, Val Accuracy: 45.45%\n",
      "Epoch [24/30], Loss: 0.2064, Accuracy: 93.02%, Val Loss: 1.7019, Val Accuracy: 48.48%\n",
      "Epoch [25/30], Loss: 0.1996, Accuracy: 93.02%, Val Loss: 1.7731, Val Accuracy: 48.48%\n",
      "Epoch [26/30], Loss: 0.1805, Accuracy: 93.02%, Val Loss: 1.9232, Val Accuracy: 51.52%\n",
      "Epoch [27/30], Loss: 0.1690, Accuracy: 94.57%, Val Loss: 2.0035, Val Accuracy: 54.55%\n",
      "Epoch [28/30], Loss: 0.1976, Accuracy: 94.57%, Val Loss: 4.1278, Val Accuracy: 54.55%\n",
      "Epoch [29/30], Loss: 0.1437, Accuracy: 95.35%, Val Loss: 4.2497, Val Accuracy: 51.52%\n",
      "Epoch [30/30], Loss: 0.1408, Accuracy: 96.90%, Val Loss: 4.3317, Val Accuracy: 54.55%\n",
      "Model saved as 'audio_model3.pth'\n"
     ]
    }
   ],
   "source": [
    "# Training with Validation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AudioModel().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Add weight decay\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for spectrogram, labels in train_loader:\n",
    "        spectrogram, labels = spectrogram.to(device), labels.to(device).float().view(-1, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(spectrogram)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "    \n",
    "    accuracy = (correct_predictions / total_predictions) * 100\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for spectrogram, labels in val_loader:\n",
    "            spectrogram, labels = spectrogram.to(device), labels.to(device).float().view(-1, 1)\n",
    "            outputs = model(spectrogram)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    \n",
    "    val_accuracy = (val_correct / val_total) * 100\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%, Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), 'D:/audio_model3.pth')\n",
    "print(\"Model saved as 'audio_model3.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_17404\\1765093638.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('D:/audio_model3.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Unconfident\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AudioModel().to(device)\n",
    "\n",
    "# Manually adjust the state dictionary\n",
    "state_dict = torch.load('D:/audio_model3.pth', map_location=device)\n",
    "\n",
    "# Remove unexpected keys related to fc1\n",
    "filtered_state_dict = {k: v for k, v in state_dict.items() if k in model.state_dict()}\n",
    "\n",
    "# Load the filtered state dictionary\n",
    "model.load_state_dict(filtered_state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "# Process and predict\n",
    "audio_path = 'selinf(2).wav'  # Replace with the path to your audio file\n",
    "spectrogram = process_audio(audio_path).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(spectrogram)\n",
    "    prediction = output.item()\n",
    "\n",
    "if prediction >= 0.5:\n",
    "    print(\"Prediction: Confident\")\n",
    "else:\n",
    "    print(\"Prediction: Unconfident\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_17404\\2762109265.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"D:/audio_model3.pth\"))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MyModel:\n\tUnexpected key(s) in state_dict: \"fc1.weight\", \"fc1.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Load the model architecture and weights\u001b[39;00m\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel()  \u001b[38;5;66;03m# Replace MyModel with your actual model class\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:/audio_model3.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Step 2: Prepare the test dataset and DataLoader\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\Gpu_\\lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MyModel:\n\tUnexpected key(s) in state_dict: \"fc1.weight\", \"fc1.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Step 1: Load the trained model\n",
    "class MyModel(AudioModel):  # Replace with your model class\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Define layers here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define forward pass\n",
    "        return x\n",
    "\n",
    "# Load the model architecture and weights\n",
    "model = MyModel()  # Replace MyModel with your actual model class\n",
    "model.load_state_dict(torch.load(\"D:/audio_model3.pth\"))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Step 2: Prepare the test dataset and DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Adjust based on your data preprocessing\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Example normalization\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=\"testr\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Step 3: Make predictions and collect labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)  # Get the class with highest score\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Step 4: Calculate metrics\n",
    "f1 = f1_score(all_labels, all_preds, average=\"weighted\")  # Adjust average for multi-class\n",
    "recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gpu_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
